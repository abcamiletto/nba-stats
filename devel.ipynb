{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA stats scraper and analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "import pathlib\n",
    "import json\n",
    "import time\n",
    "from scraper.utils import find_text_of_p_with, season_string\n",
    "from scraper.scraper import scrap_functions, get_player_seasons\n",
    "from scraper.url import get_seasons_url, get_players_url, get_player_soup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year: second half of the season - ex. 2010 --> 2009-10\n",
    "START_YEAR = 2010\n",
    "END_YEAR = 2020\n",
    "OUTPUT_DIR = pathlib.Path() / 'data'\n",
    "OVERWRITE = True\n",
    "\n",
    "# dataset textual constants\n",
    "SEASON = 'season'\n",
    "URL = 'url'\n",
    "\n",
    "# Set up constants\n",
    "BASE_URL = 'https://www.basketball-reference.com'\n",
    "SEASONS_URL = 'https://www.basketball-reference.com/leagues/NBA_{}_per_game.html'\n",
    "PLAYER_URL = 'https://www.basketball-reference.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.basketball-reference.com/leagues/NBA_2010_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2011_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2012_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2013_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2014_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2015_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2016_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2017_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2018_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2019_per_game.html', 'https://www.basketball-reference.com/leagues/NBA_2020_per_game.html']\n"
     ]
    }
   ],
   "source": [
    "# Download HTML to scrape\n",
    "seasons = get_seasons_url(START_YEAR, END_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for season in seasons:\n",
    "#     # Get the players\n",
    "#     player_urls = get_players_url(season)\n",
    "\n",
    "#     season_year = int(season.split('_')[1])\n",
    "#     season_str = f'{season_year}-{(season_year + 1) % 100}'\n",
    "    \n",
    "#     print(f'Processing season {season_year}, found {len(player_urls)} players')\n",
    "\n",
    "#     players_data = {}\n",
    "#     for url in tqdm(player_urls):\n",
    "#         # Get the player's HTML page\n",
    "#         soup_file = get_player_soup(url)\n",
    "        \n",
    "#         # Scrape HTML\n",
    "#         player = {}\n",
    "#         player[URL] = url\n",
    "#         player[SEASON] = season_year\n",
    "#         for key, function in scrap_functions.items():\n",
    "#             player[key] = function(soup_file, season=season_str)\n",
    "\n",
    "#         players_data[url] = player\n",
    "    \n",
    "#     # Save the data to a JSON file\n",
    "#     destination = OUTPUT_DIR / f'{season_year}.json'\n",
    "#     with open(str(destination), 'w') as f:\n",
    "#         print(f'Saving data to {destination}')\n",
    "#         json.dump(players_data, f, indent=4, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a player_df dataframe with columns name, season, team, url\n",
    "player_df = pd.DataFrame(columns=['name', 'season', 'team', 'url'])\n",
    "\n",
    "urls = []\n",
    "# loop through the seasons\n",
    "for season in seasons:\n",
    "    player_urls = get_players_url(season)\n",
    "\n",
    "    # add new player_urls to urls\n",
    "    urls.extend(player_urls)\n",
    "\n",
    "urls = list(set(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-14\n",
      "2016-17\n",
      "2012-13\n",
      "2015-16\n",
      "2017-18\n",
      "2016-17\n",
      "2018-19\n",
      "2017-18\n",
      "2019-20\n",
      "2013-14\n",
      "2012-13\n",
      "2010-11\n",
      "2011-12\n",
      "2013-14\n",
      "2016-17\n",
      "2012-13\n",
      "2018-19\n",
      "2015-16\n",
      "2010-11\n",
      "2014-15\n",
      "2017-18\n",
      "2011-12\n",
      "2019-20\n",
      "2011-12\n",
      "2012-13\n",
      "2015-16\n",
      "2017-18\n",
      "2013-14\n",
      "2014-15\n",
      "2019-20\n",
      "2018-19\n",
      "2010-11\n",
      "2011-12\n",
      "2016-17\n",
      "2013-14\n",
      "2012-13\n",
      "2010-11\n",
      "2011-12\n",
      "2010-11\n",
      "2011-12\n",
      "2016-17\n",
      "2015-16\n",
      "2014-15\n",
      "2013-14\n",
      "2012-13\n",
      "Saving data to data/2009-10.json\n",
      "Saving data to data/2010-11.json\n",
      "Saving data to data/2011-12.json\n",
      "Saving data to data/2012-13.json\n",
      "Saving data to data/2013-14.json\n",
      "Saving data to data/2014-15.json\n",
      "Saving data to data/2015-16.json\n",
      "Saving data to data/2016-17.json\n",
      "Saving data to data/2017-18.json\n",
      "Saving data to data/2018-19.json\n"
     ]
    }
   ],
   "source": [
    "players_data = {}\n",
    "for year in range(START_YEAR, END_YEAR):\n",
    "    players_data[str(year)] = {}\n",
    "\n",
    "# loop through first 10 urls\n",
    "for url in urls[:10]:\n",
    "    # get the player's HTML page\n",
    "    soup_file = get_player_soup(url)\n",
    "\n",
    "    # Scrape HTML\n",
    "    player = {}\n",
    "    player[URL] = url\n",
    "\n",
    "    seasons = get_player_seasons(soup_file)\n",
    "\n",
    "    for season_str in seasons:\n",
    "        year = int(season_str.split('-')[0]) \n",
    "        if year >= START_YEAR and year < END_YEAR:\n",
    "            player[SEASON] = season_str\n",
    "            print(season_str)\n",
    "            for key, function in scrap_functions.items():\n",
    "                player[key] = function(soup_file, season=season_str)\n",
    "\n",
    "            players_data[str(year)][url] = player\n",
    "\n",
    "\n",
    "# # Save the data to a JSON file\n",
    "# destination = OUTPUT_DIR / f'{START_YEAR}-{END_YEAR}.json'\n",
    "# with open(str(destination), 'w') as f:\n",
    "#     print(f'Saving data to {destination}')\n",
    "#     json.dump(players_data, f, indent=4, default=str)\n",
    "\n",
    "# save each season to a different JSON file\n",
    "for year in range(START_YEAR, END_YEAR):\n",
    "    destination = OUTPUT_DIR / f'{year-1}-{str(year)[2:]}.json'\n",
    "    with open(str(destination), 'w') as f:\n",
    "        print(f'Saving data to {destination}')\n",
    "        json.dump(players_data[str(year)], f, indent=4, default=str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
