{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA DX project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "import pathlib\n",
    "import json\n",
    "import time\n",
    "from scraper.scraper import scrap_functions\n",
    "from scraper.scraper import get_player_seasons\n",
    "from scraper.url import get_seasons_url, get_players_url, get_player_soup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from const import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PLAYERS_SUBDIR and URLS_SUBDIR if they don't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / PLAYERS_SUBDIR).mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / URLS_SUBDIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download HTML to scrape\n",
    "seasons = get_seasons_url(START_YEAR, END_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "# loop through the seasons\n",
    "for season in seasons:\n",
    "    # get season number from season url contained in season\n",
    "    season_str = season_string_from_url(season)\n",
    "\n",
    "    # if season urls have already been downloaded, skip\n",
    "    if pathlib.Path(OUTPUT_DIR / URLS_SUBDIR / f'{season_str}.pkl').exists():\n",
    "        # retrieve the urls from the pickle file\n",
    "        with open(OUTPUT_DIR / URLS_SUBDIR / f'{season_str}.pkl', 'rb') as f:\n",
    "            urls.extend(pickle.load(f))\n",
    "    \n",
    "    else:\n",
    "        player_urls = get_players_url(season)\n",
    "\n",
    "        # save player_urls to a file in URLS_DIR using pickle\n",
    "        # if file does not exist, create it\n",
    "        with open(OUTPUT_DIR / URLS_SUBDIR / f'{season_str}.pkl', 'wb') as f:\n",
    "            pickle.dump(player_urls, f)\n",
    "\n",
    "        # add new player_urls to urls\n",
    "        urls.extend(player_urls)\n",
    "\n",
    "urls = list(set(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/1959-60.json\n",
      "Saving data to data/1960-61.json\n",
      "Saving data to data/1961-62.json\n",
      "Saving data to data/1962-63.json\n",
      "Saving data to data/1963-64.json\n",
      "Saving data to data/1964-65.json\n",
      "Saving data to data/1965-66.json\n",
      "Saving data to data/1966-67.json\n",
      "Saving data to data/1967-68.json\n",
      "Saving data to data/1968-69.json\n",
      "Saving data to data/1969-70.json\n",
      "Saving data to data/1970-71.json\n",
      "Saving data to data/1971-72.json\n",
      "Saving data to data/1972-73.json\n",
      "Saving data to data/1973-74.json\n",
      "Saving data to data/1974-75.json\n",
      "Saving data to data/1975-76.json\n",
      "Saving data to data/1976-77.json\n",
      "Saving data to data/1977-78.json\n",
      "Saving data to data/1978-79.json\n",
      "Saving data to data/1979-80.json\n",
      "Saving data to data/1980-81.json\n",
      "Saving data to data/1981-82.json\n",
      "Saving data to data/1982-83.json\n",
      "Saving data to data/1983-84.json\n",
      "Saving data to data/1984-85.json\n",
      "Saving data to data/1985-86.json\n",
      "Saving data to data/1986-87.json\n",
      "Saving data to data/1987-88.json\n",
      "Saving data to data/1988-89.json\n",
      "Saving data to data/1989-90.json\n",
      "Saving data to data/1990-91.json\n",
      "Saving data to data/1991-92.json\n",
      "Saving data to data/1992-93.json\n",
      "Saving data to data/1993-94.json\n",
      "Saving data to data/1994-95.json\n",
      "Saving data to data/1995-96.json\n",
      "Saving data to data/1996-97.json\n",
      "Saving data to data/1997-98.json\n",
      "Saving data to data/1998-99.json\n",
      "Saving data to data/1999-00.json\n",
      "Saving data to data/2000-01.json\n",
      "Saving data to data/2001-02.json\n",
      "Saving data to data/2002-03.json\n",
      "Saving data to data/2003-04.json\n",
      "Saving data to data/2004-05.json\n",
      "Saving data to data/2005-06.json\n",
      "Saving data to data/2006-07.json\n",
      "Saving data to data/2007-08.json\n",
      "Saving data to data/2008-09.json\n",
      "Saving data to data/2009-10.json\n",
      "Saving data to data/2010-11.json\n",
      "Saving data to data/2011-12.json\n",
      "Saving data to data/2012-13.json\n",
      "Saving data to data/2013-14.json\n",
      "Saving data to data/2014-15.json\n",
      "Saving data to data/2015-16.json\n",
      "Saving data to data/2016-17.json\n",
      "Saving data to data/2017-18.json\n",
      "Saving data to data/2018-19.json\n",
      "Saving data to data/2019-20.json\n",
      "Saving data to data/2020-21.json\n",
      "Saving data to data/2021-22.json\n"
     ]
    }
   ],
   "source": [
    "players_data = {}\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    players_data[str(year)] = {}\n",
    "\n",
    "# loop through first 10 urls\n",
    "for url in urls[:10]:\n",
    "    # get the player's HTML page\n",
    "    soup_file = get_player_soup(url)\n",
    "    seasons = get_player_seasons(soup_file)\n",
    "\n",
    "    for season_str in seasons:\n",
    "        # Scrape HTML\n",
    "        player = {}\n",
    "        player[URL] = url\n",
    "        \n",
    "        year = season_number_from_string(season_str)\n",
    "        if year >= START_YEAR and year <= END_YEAR:\n",
    "            player[SEASON] = season_str\n",
    "            \n",
    "            for key, function in scrap_functions.items():\n",
    "                player[key] = function(soup_file, season=season_str)\n",
    "\n",
    "            players_data[str(year)][url] = player\n",
    "\n",
    "\n",
    "# save each season to a different JSON file\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    destination = OUTPUT_DIR / f'{season_string_from_number(year)}.json'\n",
    "    with open(str(destination), 'w') as f:\n",
    "        print(f'Saving data to {destination}')\n",
    "        json.dump(players_data[str(year)], f, indent=4, default=str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
